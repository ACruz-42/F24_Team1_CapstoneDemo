## Intro

The Institute of Electrical and Electronics Engineers (IEEE) hosts a yearly hardware competition at its Southeast Conference (SECON) for students to compete in.  Tennessee Technological University has competed in the IEEE SECON hardware competition in the past but has often had issues undiscovered until the actual competition.

The 2025 competition rules and limitations will be explained in detail below but, in short, its challenge takes place over 3 minutes and involves collecting and separating two types of 40mm icosahedrons (astral material) randomly distributed on the field. Geodinium, one type of astral material, is slightly magnetic and heavier than Nebulite, the other type. The challenge is to sort these two types of astral material into two designated containers, and then bring those containers to a randomly chosen area on the board. Additionally, there is a dark roofed “cave” area with further Geodinium and Nebulite.  The board is relatively simple with few obstacles beyond the containers and astral materials to navigate around, and as such, the goal is to focus on reliability followed by an extensive testing period. There are two types of matches, qualification matches in which the eight teams with the highest scoring qualification matches, and elimination matches where two teams directly compete against each other’s scores.

The point distribution greatly rewards being able to correctly and reliably sort Geodinium and Nebulite into their corresponding containers so, by reliably gaining these points and unreliably gaining others, performance can be optimized for both the qualification matches and elimination matches [1].

## Problem

### Motor Control

During the competition, the robot will need to move quickly to collect as much astral material as possible within the allotted 3-minute timeframe. Therefore, one of the focal points of previous design teams for the robot was to ensure precise motor control for efficient power usage and movement while also protecting other subsystems from disruptive effects produced by the motor. 

The team that worked on the previous robot base used DC motors due to their increased control capabilities and starting torque in comparison to AC motors. However, they were concerned with the back electromotive force that moves in the opposite direction of the motor spin. The back emf can interfere with low-power, highly sensitive logic circuits. This problem is also present when the motor stops or quickly slows down [7]. During the match, the robot will be required to quickly move across the area of play and collect astral material automatically. The robot will need to stop and start again numerous times as it makes its way around the field. The back emf generated by continual stopping and starting of the motors would be harmful to the control circuitry, generating noise. It is critical to reduce this emf noise generated by the motor as much as possible for optimal communication between the controllers and mechanisms.  

### Sensors for localization and navigation

For the competition, the robot will need to know where it is with respect to its goal and its boundaries. There are multiple items that will help the robot with navigation and localization. These items include walls and April Tags as shown in Figure 1. The walls of the arena, 3.5 in. tall, are relatively small compared to the maximum size of the robot (12 in.) and the height of the cave walls (24 in.). One April Tag is placed on each of the four sides of the game arena. Three of the tags are outside the cave, while one is inside the cave. The tag inside the cave is across from the entrance. The tag across from the cave tag is for designating the correct rendezvous pad to deliver the containers to. To the right of each of the containers is a tag.

The sensors used to localize the robot need to be able to detect the walls of the arena, April Tags, and the spatial geometry of the robot. The robot will need to know the limits of where it can be and where within these limits it is located. The walls set a hard limit on where it can be. Along with the walls, April Tags can be used to locate itself with respect to where it expects to be. Furthermore, a sensor will be used to determine its geometry in the arena [1]. 

![image](https://github.com/ACruz-42/F24_Team1_CapstoneDemo/blob/3b0272b824db69aba49f735e246663ff42dbf47a/Reports/Photos/Project%20Proposal/Mining%20Mayhem%20Field%20Opposite%20Corner.png)

Figure 1 shows the game field without the cave covered [2].

### Navigation and Master Control

The robot must take in data from its sensors, keep track of its own location, determine the location of the astral material around it, plot a path taking into consideration certain static objectives (such as entering the cave, and the containers for the astral materials), send the corresponding signal to the motors, and then automatically shut down after three minutes.

### Object and Line Detection

During the competition the robot will need to be able to detect the white lines that denote the starting area, the 5 telemetry zones, and the path in and out of the cave, and interpret the lines in a useful way pertaining to the competition. The robot must also be able to detect the astral material and the shipping containers. The robot must be able to detect the white lines, the astral materials, and the shipping containers in both illuminated and dark environments. The robot must be able to mark and remember the locations of the shipping containers, the astral materials, and the white lines so that it can plan an efficient path to collect the astral material and place it in the shipping containers.

### Emergency Stop

As stated in the game Rule R04, each robot competing in the match must have a clearly marked emergency stop button/switch. This is also a safe method of ethics to follow for autonomous machines as stated in NFPA 79-10: NFPA 79 Section 10 [6] outlines Emergency-Stop system Requirement for industrial machinery. The Robot’s E-stop will be based on the code identified in the reference.

## Specifications

1)	The robot shall act autonomously. Rule G08 [2].
2)	The team shall design a robot to earn as many points as possible [2].
    1)	The robot may score 5 points by moving out of the landing pad [2].
    2)	The robot may score 5 points by moving out of the landing pad within 3 seconds of the Start LED [2].
    3)	The robot may score 15 points by moving into the cave [2].
    4)	The team may score 5 points by entering the Promotional Design Competition [2].
    5)	The robot may score 60 points for putting both containers in the correct zone [2].
    6)	The robot may score 34 points by completely supporting all Astral Material [2].
    7)	The robot may score 80 points by putting all Nebulite in its container [2].
    8)	The robot may score 108 points by putting all Geodinium in its container [2].
    9)	The robot may score 40 points for putting the Team Beacon in the Beacon Mast [2].
3)	The robot shall fit within a cube with 12 inches per side at the start of the match. Rule G04 [2].
4)	The robot shall be able to detect the walls of the arena, walls of the cave, and cave entrance. Rule S03 states that if the robot causes field damage, the team will be penalized. Detecting walls stops the robot from causing damage by moving into it [2].
5)	The robot shall stay on the game field. Rule S01 [2].
6)	The robot may have a single, simple electrical start button that shall be clearly labelled. The operation of this button shall prevent the robot from moving for five seconds. Rule R03 [1].
7)	The robot shall have a clearly labelled emergency stop button that immediately halts all functions safely and quickly. Rule R05 [1].
8)	The robot shall have a source of identification clearly associating it with Tennessee Technological University. Rule R10 [1].
9)	The robot may use light sources that shall not strobe, be excessively bright, or pose safety issues. Rule R17 [1].
10)	The robot shall cease operation of all its units after the allotted 3-minute timeframe of play is over, and no game element positions can be disturbed after time expires. Rule G07 [2]. 
11)	The robot shall account for background interference in the competition environment. Rule R13 [1].
12)	The robot shall sustain power of 30V maximum. Rule R16 [1].

## Constraints

1)	The robot shall read April Tags to help with Specifications 1, 2e, 2g, 2h, and 2i. 

2)	The robot shall have a user manual that explains functionality and design intent for each subsystem.

3)	 The robot shall adhere to applicable requirements in standard IEC 60204-1 pertaining to electrical supply, electromagnetic compatibility, emergency stop, and control circuit protection. The power-based and electromagnetic interactions of components in the motor control subsystem will need to adhere to this standard [11].

## Survey of Solutions

### Motor Control

In order to adhere to the specifications as given from IEEE and as required from the physical electronics, an ideal solution for motor control would be a self-regulating, efficient, and protective system that can reduce adverse interference effects and relay commands as quickly as possible. To reduce the noise generated by the motors and environment, there are several techniques in use that can be applied to the robot, including PID tuning, filtering, and shielding [7]. 

PID tuning is a method that involves continuous monitoring of the current state of the motor and adjusting the aspects of that state as required. A motor decoder is a component that can assist with PID tuning. In between voltage pulses as power is being applied to the motor, the noise that the decoder can detect comes from back EMF. The decoder is then able to report changes in noise to the PID controller based on the back emf reading [8]. That way, if there is a drastic increase in speed and resulting back emf, the PID controller can regulate the speed and bring it back down, reducing the effect of the back emf on the control circuitry. 

For the noise filtration method, a capacitor is often used as a short circuit for changing, high frequency currents, which will be present if the robot continuously starts and stops the motors. A single capacitor can be attached to the motor terminals to lower noise along the wires, but multiple capacitors connected to the motor terminals and the casing can act as a diverting path for exterior noise emanating from the motor into its environment. A note to keep in mind for this method is that ceramic disc capacitors must be used, as electrolytic capacitors can explode when used for this purpose [7]. 

Even though the previous methods help reduce noise from the motors, there is still the environmental noise reduction that needs to be addressed. Data Acquisition and Test Systems is an application that uses the following methods, placing an emphasis on reducing coupled noise from external sources [10]: cable shielding, using twisted pair cables, isolating signals, ground wires properly, organizing wiring pathways, and using anti-aliasing filters. Each of these methods can be implemented into the electrical design of the robot, and they should be considered for the efficiency of autonomous communications and the safety of the equipment and surrounding environment. 

### Sensors for localization and navigation

According to the works presented in Modular IEEE Robot, there were two sets of sensors used for localization and navigation. The Grove Ultrasonic Sensor is used for navigation. This sensor sends out a high frequency sound pulse, collects the reflected pulse, and calculates the time between the output and input [13]. The Adafruit LSM6DSOX + LIS3MDL breakout board is used for orientation. It can detect changes in linear and angular acceleration and has a magnetometer for detecting the Earth’s magnetic field [14]. A single orientation sensor is used while an ultrasonic sonic sensor is used on each side of the past robot [12].

Another solution to navigating the environment is using a Light Detection and Ranging (LiDAR) device. LiDAR is commonly used to map out an environment to make it navigable by an autonomous unit. LiDAR’s have high resolution at a relatively large distance. A LiDAR could be used for getting a map of the game field. However, LiDAR has a minimum detection range of 20 cm (7.874 in.). When the robot is close to a wall, LiDAR data will have to be filtered out, and a different sensor will need to be used. LiDAR placement on the robot will have to be careful, so that it can detect as much as possible in its 360-degree field of view, given our size requirements. While LiDAR uses a laser, it is human safe and in the infrared range [20].

April Tags can be read easiest with a camera. An RGB-D camera can read in both color and depth. The tags have both color difference and depth difference; however, the depth difference is only 0.4 mm. Using the color difference should be easily implemented and the depth can be used to filter noise out of the background. The data read in from the camera will have to be carefully analyzed and filtered to be able to read the tag from multiple different directions and angles.

A CMOS camera would be better than a CCD camera for its size and power consumption but have a higher noise level [15]. Choosing a resolution to pick out the tags is decided by finding the farthest distance from a tag the robot will have to read and computing the size needed to tell the difference between blacks squares and white squares in the tag [16].

### Navigation and Master Control

The particulars of the navigational and master control system will be dependent on the sensors, and exact mechanisms used to manipulate the game field. Currently, two sets of solutions have been explored as the most viable considering the time limitations regarding construction. The base modular robot mentioned previously uses an Arduino Mega, along with two ESP 32 microcontrollers attached to sensors. It uses a modified A* algorithm for pathfinding that can be adapted for the particulars of the competition. Upgrading the Arduino Mega to a Raspberry Pi would provide a considerable amount of processing power while mostly allowing continued usage of their programming. This upgrade would also enable the usage of computer vision and machine learning to enable the robot to detect objects more specifically, and potentially allow the usage of Apriltag-based localization. The usage of Raspberry Pi for computer vision is ubiquitous to the point of having a dedicated page on the Raspberry Pi organization’s website. [3]. The usage of Apriltag-based localization would require more coding, as there is literature on landmark-based localization, but nothing as specific as AprilTag-based localization.

However, the Jetson Nano utilizes ROS and QR-code based localization has been achieved on ROS [3]. A Jetson Nano is also more powerful with AI-related workloads as it contains a graphics processing unit that supports CUDA and is designed for AI-related tasks [4] whereas a Raspberry Pi does not. A Raspberry Pi might have difficulties using computer vision, processing navigation algorithms, and controlling sensors and motors even with ESP-32 microcontroller support. Making use of a Jetson Nano would, however, necessitate an overhaul of the modular robot base’s code, and possibly even the discarding of it entirely. Anecdotal experiences from mentors and fellow students have depicted ROS as difficult to learn and fully utilize.

### Object and Line Detection

The robot will need some kind of sensors fitting for the task of object and line detection, as of right now the leading candidate is an RGB-D camera. For line detection once input from the sensors is obtained then lines can be detected from the input image in 1 of 2 ways. The first method is the Hough Transform; the first thing to do is divide the image into a series of cells much like a grid, the Hough Transform uses the equation ρ = r cos(θ) + c sin(θ) [17], which transforms each point into ρ,θ space, now that the image is in polar coordinates, each point is represented as a sine wave, and each time the sine waves intersect represents a line that crosses both points, so if we add up all intersections in each cell the cell with the most intersections will correspond to the strongest line in the image. The convolution method involves placing masks over the image; a mask is placed over each pixel and by tracking how well the mask fits across each pixel for multiple pixels in a row using an edge tracking software we can detect if any lines are present. [18]

The robot can also use the RGB-D camera for object detection. Object detection is a task in the ECE field that involves picking out objects from some image, classifying them and localizing them. Classification is what something is, a tree, a rock, a man, or anything else, it classifies the object into a category, for this step our project will be simpler than many object detection program as we only have 2 categories of objects to classify. Localization is where an object is, what is around it and what physical space it occupies. The are many ways to go about the task of object detection, most methods fall into 1 of 2 categories. 2 stage detection works in 2 steps, first a series of uniform regions are proposed called anchors, the anchors are then evaluated on how likely a relevant object is in that anchor, then put through the classification and localization processes. Some examples of 2 stage detection are R-CNN (Regions with Convolution Neural Networks) and Fast R-CNN. [9,19] 1 stage detection involves merging the tasks of classification and localization into a single pass, some examples of this are YOLO (You Only Look Once) and SSD (Single Shot MultiBox Detector). [9]

## Summarizing the Problem

The IEEE SECON Hardware Competition is an opportunity for each of the team members to demonstrate their knowledge in the design and construction of an autonomous robot suited to collect and sort specified material under a given timeframe. The project will challenge the group to search for tools and methods that may not be emphasized in prior university courses. The competition presents an array of chances to earn points, prompting the creation of the following subsections of the robot to obtain the most points as efficiently as possible: Motor Control, Localization and Navigation Sensors, Navigation and Master Control, Object and Line Detection, and an Emergency Stop. The team will continue developing each of these subsections to address the given specifications and adhere to the design constraints.

## Unknowns

Going into this project, the team had a basis for the robot chassis from a prior capstone team. However, they did not pursue a design for the collection and sorting mechanisms that the chassis would support. Therefore, a large unknown at this stage of the project regards the determination of the most efficient system for collecting astral material and sorting it into the correct containers. Additionally, the number of each type of astral material within and outside the cave is known, but the positions of each are not known. Coordinating with the Mechanical Engineering team on determining the best approach for navigating to and correctly sorting the astral material will be an initial step in satisfying this unknown. The given specifications for point totals [2] will be taken into consideration for creating a system that can obtain the most points in the shortest timeframe, prioritizing certain point totals as necessary. For example, if it is determined that actively attempting to place the Team Beacon significantly hinders the robot from collecting most of the available astral material, then the mechanisms in place may be adapted to reflect the high priority of material collection. Simulating the functionality of the robot using a system such as ROS would enable more rapid testing and revision of the design. 

With the final examination of the robot’s performance being a competition, another unknown will be how well other teams will score. It will be difficult to determine how successful other competitors will be leading up to the competition, but the team can use projects from prior years as broad examples of the methodology used in meeting their objectives and  specifications. Heavily utilized programming tools, simulation software, and circuit components that provided clear, measurable benefits to previous teams would serve as great resources for replicating such success.

## Measurements of Success

This section will define the effectiveness or success of the constraints and specifications listed above. If the robot does not meet 5 of the listed constraints it will be considered a critical failure. Some of the specifications stated may alter as the project progresses, but this will be at the discretion of ultimately the customer on a case basis. The success of the robot will depend on the completion of the project within the course deadline and before the competition deadline. We hope as a team that this robot will serve as a solid foundation to other students who come across a similar task. Although the goals are set and it is desirable to fully realize them, there are alternative investments that are being made for future generations in the completion of this project.

The robot will do at least 20 test runs utilizing the competition rules on a recreation of the game board. If the robot stays within the game board's boundaries and does not require outside assistance to continue moving at any point, specifications 1, 4, and 5 will be considered satisfied. If the robot earns at least 80% of as many points as designed to, specification 2 will be considered satisfied. If any lights the robot may possess do not strobe and do not disorient the tester, specification 9 will be considered satisfied.

In order to satisfy specification 1 the robot will need to be able to detect both white lines against a dark background and purple objects against a dark background, both in dark and light environments.[2] This will be tested by the robot being placed on the game field, both inside and outside the cave, and then preforming the tasks of line detection and object detection at least 10 times, with the robot being moved and each time and the astral material being randomly placed each time. The robot will need to do this with at least 90% accuracy, this will be defined as the robot being able to accurately place each piece of material and each line within 1.5 inches of its actual position. 

The robot will be measured in its starting position using a ruler after every physical addition that changes its volume, and if the robot fits within a cube with 12 inches per side, then specification 3 will be considered satisfied.

The robot’s starting button will be pressed 20 times while the robot is in the off state. If the robot does not move for five seconds, and then begins its tasks, specification 6 will be considered satisfied. 

The robot’s emergency stop button will be pressed 20 times at different points in its operation (e.g., starting, ending, inside of the cave, sorting astral material, picking up containers, and other points where the tester determines to be of note). If the robot successfully immediately stops each time, specification 7 will be considered satisfied.

The robot will have a label on the chassis to identify it as from TN Tech. If possible and deemed appropriate, it can be on any outer extremity of the final robot product. Specification 8 will be considered satisfied.

The robot will be allowed to run for the full duration of the game. After the 3 minutes have passed, it will be tested to see if the robot stopped in time. If successful 10 times, specification 10 will be considered satisfied.

Specification 11 will be tested by adding noise outside the game field but possible within the sight of the robot’s sensors. If the robot filters through the noise from 10 separate stimuli, specification 11 will be considered satisfied.

### Ethics and Responsibility

This design team shall follow not only the rules of the competition Mining Mayhem, but also the National Electrical Code safety procedures, associated National Fire Protection Agency procedures, and abide by piracy laws. 

If the team is able to complete the robot and place well in the competition, Tennessee Tech will receive recognition on a regional scale, which will serve to benefit the outreach capabilities of the university and potentially bring in more students to participate in future engineering projects. However, the team is also held to a high standard, representing Tech on a technical and professional level.

### Resources

Table 1 is the suggested Bill of Materials (BOM).

|Item|Cost per Item|Quantity|Total Cost for Item|
| :- | :- | :- | :- |
|Ultrasonic Sensor|~$6|4|$24|
|Accelerometer and Gyroscope|~$40|1|$40|
|Lithium Iron Phosphate Battery|$91.43|1|$91.43|
|Lithium Iron Phosphate Battery Charger|$129.27|1|$129.27|
|Navigation & Master Control|~$250|||
|Brushless Motors|~$60.00|4|$240.00|
|Microcontroller|~$30.00|2|$60.00|
|Motor Drivers|~$20.00|2|$40.00|
|Intel® RealSense™ Depth Camera D415</h1>|~$272.00|1|$272.00|
|Motor Encoders|~$23.00|1|$23.00|
|Total|||$1169.70|

Timeline

![image](https://github.com/ACruz-42/F24_Team1_CapstoneDemo/blob/a78e467d4dc7ae5cd5a58d4d5bec27a2e4cb2b21/Reports/Photos/Project%20Proposal/Screenshot%202024-09-27%20141156.png)


- Personnel
  - Sean Borchers - Electrical Engineering major with a Mechatronics concentration. Proficient in CAD software (AutoCAD, Inventor, SolidWorks), MATLAB, C++
  - Alex Cruz – Electrical Engineering major. Proficient in Python and C++. Some familiarity with machine learning.
  - Sam Hunter – Computer Engineering major. Proficient in C++.
  - Dakota Moye – Electrical Engineering major with minors in Physics and Math. Proficient in C++, MATLAB, Arduino IDE. Some experience with Python, Inventor, 3D Printing, PuTTY.
  - Alejandro Moore – Electrical Engineering major Proficient in C++, C#, C, Assembly, Python, AutoCAD, PLC, and Altium. PCB Design experience and BNC/Ethernet cable installation experience.

- Contributions
  - Sean Borchers – Motor Control, Unknowns, Summarizing the Problem
  - Alex Cruz - Intro, Navigation and Master Control, Measures of Success
  - Sam Hunter - Object and Line Detection
  - Dakota Moye – Sensor for Navigation and Location
  - Alejandro Moore - Power Management, Emergency Stop, Timeline (Gantt Chart)

## Works Cited

1.	“Mining Mayhem – Game Manual 1.” Version 1.1, Apr. 2024. Accessed: Sep. 2024. [Online]. Available: https://docs.google.com/document/d/1hTvIeRj649eyGU8oWLR_yD-mYgayySX7tRQBbetUCqc/edit 

2.	“Mining Mayhem – Game Manual 2” Version 1.1.3, Aug. 2024. Accessed: Sep. 2024. [Online]. Available: https://docs.google.com/document/d/1fN7bsJFpCJur66JkueRHXtlybt0m7QSY4Nn62lHAnrc/edit 

3.	Raspberry Pi Foundation.  “Machine Vision.” Accessed: Sep. 2024. [Online]. Available: https://projects.raspberrypi.org/en/pathways/machine-vision 

4.	S. Lee, G. Tewolde, J. Lim, et al. “QR-code based Localization for Indoor Mobile Robot with Validation using a 3D Optical Tracking Instrument” Feb. 2023. Accessed: Sep. 2024. [Online]. Available: https://jrkwon.com/wordpress/wp-content/uploads/2023/02/AIM-2015-Final.pdf 

5.	NVIDIA. “NVIDIA Jetson Nano”. Accessed: Sep. 2024. [Online]. Available: https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-nano/product-development/ 

6.	J3, “DC MOTORS —Against Back-EMF,” Medium, Dec. 05, 2020. Accessed: Sep. 2024. [Online]. Available: https://medium.com/jungletronics/dc-motors-against-back-emf-589d8ed174cc

7.	“Tony’s Train Exchange,” News & Resources, Sep. 12, 2018. Accessed: Sep. 2024. [Online]. Available: https://tonystrains.com/news/dcc-motor-control-with-back-emf-and-p-i-d/

8.	A. Corry, G. Mostyn, and J.-Y. Michel, “Noise reduction in integrated circuits and circuit assemblies,” Jul. 15, 1997. Accessed: Sep. 2024. [Online]. Available: https://patents.google.com/patent/US5649160A/en

9.	S. Schrems, “Top 8 Ways to Deal with Noise in Data Acquisition and Test Systems,” genuen.com. Accessed: Sep. 2024. [Online]. Available:  https://www.genuen.com/blog/top-8-ways-to-deal-with-noise-in-data-acquisition-and-test-systems/

10.	Safety of machinery - Electrical equipment of machines. 2016. Accessed: Sep. 2024. [Online]. Available: https://webstore.iec.ch/en/publication/26037

11.	L. Chapman, R. Crews, I. Hoese, I. Jennings, A. Kennedy, and M. Olson. “Experimental Analysis: Specification-14.” Control-Sensing-Wireless-Charging-Robot. GitHub.com. Accessed: Sep. 2024. [Online.] Available: https://github.com/lchapman42/Control-Sensing-Wireless-Charging-Robot/blob/main/Documentation/Experimental%20Analysis/Experimental%20Analysis.md#specification-14---frame-weight-requirements

12.	L. Chapman, R. Crews, I. Hoese, I. Jennings, A. Kennedy, and M. Olson. “Orientation-Signoff.” Control-Sensing-Wireless-Charging-Robot. GitHub.com. Accessed: Sep. 2024. [Online.] Available: Control-Sensing-Wireless-Charging-Robot/Documentation/Signoffs/Orientation-Signoff.md at main · lchapman42/Control-Sensing-Wireless-Charging-Robot (github.com)

13.	L. Chapman, R. Crews, I. Hoese, I. Jennings, A. Kennedy, and M. Olson. “ReidCrews-Signoff-Location.” Control-Sensing-Wireless-Charging-Robot. GitHub.com. Accessed: Sep. 2024. [Online.] Available: Control-Sensing-Wireless-Charging-Robot/Documentation/Signoffs/ReidCrews-Signoff-Location.md at main · lchapman42/Control-Sensing-Wireless-Charging-Robot (github.com)

14.	“Imaging Electronics 101: Understanding Camera Sensors for Machine Vision Applications.” edmundoptics.com. Accessed: Sep. 2024. [Online.] Available: Imaging Electronics 101: Understanding Camera Sensors for Machine Vision Applications (edmundoptics.com)

15.	“Machine Vision Camera Selection Guide.” mech-mind.com Accessed: Sep. 2024. [Online.] Available: Machine Vision Camera Selection Guide (mech-mind.com)

16.	W. Wang, A. Tan-Torres, and H. Hamledari, “Lecture #06: Edge Detection.” Accessed: Sep. 2024. [Online]. Available:  http://vision.stanford.edu/teaching/cs131_fall1718/files/06_notes.pdf

17.	“Line Detection,” Ed.ac.uk, 2024. Accessed: Sep. 2024. [Online]. Available: https://homepages.inf.ed.ac.uk/rbf/HIPR2/linedet.htm

18.	K. He, G. Gkioxari, P. Dollar, and R. Girshick, “Mask R-CNN,” IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 1–1, 2018. Accessed: Sep. 2024. [Online]. Available: https://doi.org/10.1109/tpami.2018.2844175

19.	“What is Object Detection in Computer Vision?,” GeeksforGeeks, May 10, 2024. Accessed: Sep. 2024. [Online]. Available: https://www.geeksforgeeks.org/what-is-object-detection-in-computer-vision/

20.	“RPLIDAR A2.” slamtec.com. Accessed: Sep. 2024. [Online.] Available: RPLIDAR-A2 Laser Range Scanner Parameters|SLAMTEC

